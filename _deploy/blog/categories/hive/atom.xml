<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hive | MayCoder]]></title>
  <link href="http://code6.github.com/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://code6.github.com/"/>
  <updated>2012-12-16T23:01:24+08:00</updated>
  <id>http://code6.github.com/</id>
  <author>
    <name><![CDATA[Code6]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[hive compile and apply patch]]></title>
    <link href="http://code6.github.com/blog/2012/09/19/hive-compile-and-apply-patch/"/>
    <updated>2012-09-19T13:38:00+08:00</updated>
    <id>http://code6.github.com/blog/2012/09/19/hive-compile-and-apply-patch</id>
    <content type="html"><![CDATA[<p>p(date). 2012-09-20</p>

<p>回归来的第一篇，另外还有好几篇躺在 @published:false@ 中，还没搞好 :-(</p>

<p>这个礼拜最大的收获是编译啥啥啥还是蛮简单的，又没叫你去改代码。有些东西动手尝试一下，往往会发现没有多难，不能停留在口上上说说而已。看到 "一段相关但不是很切题的话":http://blog.liancheng.info/wq-2008-1/ :</p>

<p>bq. 不可忽视的一点是：在技术性团体里，做永远比说要来的有效。在想法成熟之前就贸然游说，往往会招致相反的效果。大家都是工程师，不会贸然接纳陌生事物。如果自己都还没有想清楚就开始大肆游说，往往会被大家提出的实际的工程问题驳斥地体无完肤。当你哑口无言之时，大家也已经对你的方案产生了难以磨灭的 “不靠谱”的第一印象，这时要再想咸鱼翻身，可就没那么容易了。</p>

<p>bq. 相对的，首先自行查阅文档资料并进行试验，制作demo，通过试验发现和解决实际出现的问题。在想法基本成型时，和个别观念开放的同仁进行探讨，这时往往可以发现大量之前自己没有考虑到的问题，再转而细化方案。这个过程反复迭代几次之后，方案和demo逐渐成熟，同时也潜移默化地达到了传教的目的。等到方案完全成熟之后，再拿出实际可工作的demo开始游说，这时自然就成竹在胸了。</p>

<p>进入正题，这里主要是参考几篇文章，说说编译hive和打补丁, 或者说贴贴链接= =</p>

<p>h3. hive 代码编译</p>

<p>"install hive on linux":http://railsbuilder.blogspot.com/2010/06/installing-hive-on-linux.html<br/>
"使用ant编译hive":http://blog.csdn.net/scutshuxue/article/details/5915689<br/>
"hive打补丁编译hive":http://blog.csdn.net/cfy_yinwenhao/article/details/6977882</p>

<p>这里我在ubuntu上操作的, 编译hive需要依赖 JAVA, ANT (这里不依赖hadoop原因是ant会自动去下载hadoop)。
安装好后export @JAVA_HOME@ , @ANT_HOME@ 变量 (我直接加入.bashrc当中了)。
接下来去官网下载hive源码包。这里我使用的是 @hive.0.9.0@ 。 解压之后进入src目录，需要修改一些配置:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>build.properties </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='properties'><span class='line'><span class="na">hadoop.version</span><span class="o">=</span><span class="s">1.0.1&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="na">&lt;p&gt;主要是facebook被墙了，镜像地址可参考 http</span><span class="o">:</span><span class="s">//www.apache.org/dyn/closer.cgi/hadoop/core/</span>
</span><span class='line'><span class="na">hadoop.mirror1</span><span class="o">=</span><span class="s">http://mirror.bjtu.edu.cn/apache&lt;br/&gt;</span>
</span><span class='line'><span class="na">hadoop.mirror2</span><span class="o">=</span><span class="s">http://mirror.bjtu.edu.cn/apache&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="err">&lt;p&gt;以下的修改主要是看到镜像中的hadoop版本已经没有默认的版本号了，估计是不支持了?故改成包含的版本号。</span>
</span><span class='line'><span class="na">hadoop-0.20.version</span><span class="o">=</span><span class="s">0.20.2</span>
</span><span class='line'><span class="na">hadoop-0.20S.version</span><span class="o">=</span><span class="s">1.0.1 (S的版本不知有没有啥区别)&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="err">&lt;p&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>接下来执行 @ant package@ 即开始编译了, 结果默认会放在 @build/dist@ 目录中，使用 @-Dtarget.dir=xxx@ 可以更新目标位置。</p>

<p>如果BUILD FAIL的话，一般参考提示可以看出是哪里出了问题(遇到比较多还是被墙的问题= =)。</p>

<p>h3. hive 单元测试</p>

<p>"hive中的单元测试":http://www.oratea.net/?p=632
"hive中的test case":http://blog.csdn.net/bupt041137/article/details/6553770
"QtestUtil.java":http://blog.csdn.net/bupt041137/article/details/6553760
"hive unit test":http://blog.csdn.net/bupt041137/article/details/6553760</p>

<p>hive 中的 "单元测试":https://cwiki.apache.org/Hive/howtocontribute.html#HowToContribute-UnitTests 还是蛮有趣的，既有传统的单元测试代码，又可以批量执行查询脚本判断结果是否一致:</p>

<p>bq.  在hive中会有大量的.q的文件即执行测试的query文件，.q.out是测试的结果文件。 进行新测试后产生的结果和标准的q.out一致则表示测试成功。
具体来说，src/ql/src/test/queries下面是测试用例，clientpositive是运行成功的用例，clientnegative是运行失败，返回非0的用例。 src/ql/src/test/results 下面是测试用例对应的输出结果。 如src/ql/src/test/queries/case_sensitivity.q对应的输出结果是src/ql/src/test/results/case_sensitivity.q.out 。</p>

<p>使用 @ant test@ 运行单元测试，不过此命令要跑好多测试， "光TestCliDriver一项就要跑好几个小时":https://builds.apache.org/job/Hive-trunk-h0.21/1671/testReport/org.apache.hadoop.hive.cli/TestCliDriver/ , 我们可以</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>ant test </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>运行所有的正面的测试
</span><span class='line'>ant test -Dtestcase=TestCliDriver&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>运行特定的测试，以groupby为例子
</span><span class='line'>ant test -Dtestcase=TestCliDriver -Dqfile=groupby1.q</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>使用 @-Doverwrite=true@ 可以覆盖结果文件，后面有新的更新想测试表现是否一致的时候就可以使用了, 通常在我们增加新的udf的时候可以这么来生成测试数据。</p>

<p>h3. hive代码打补丁</p>

<p>下载了 @hive 0.9.0@ 稳定版，不过发布之后还是有若干bug，这时搜索一下 @hive jira@ 一般能看到相应ticket，比如 "这个substr遇到UTF8的问题":https://issues.apache.org/jira/browse/HIVE-2942 。一般打的补丁会走一个 @publish-review-apply@ 的 "过程":http://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;cad=rja&amp;ved=0CEIQFjAD&amp;url=http%3A%2F%2Fwww.cs.pdx.edu%2F~bart%2Fpapers%2Fpicmet-patch.pdf&amp;ei=g0VbUK6LDITqmAW2tICQCQ&amp;usg=AFQjCNEoh5FKA_8KNBa5-Pr-gMaQPHS9GA&amp;sig2=ISd1WAautuWBKKfc_Vv3lg ，准确解决问题的补丁会被加入版本库中，而临时补丁一般不会采用。作为用户，有时难免得打一些这样的补丁。打补丁也很方便，下载社区提供的补丁之后，使用 @patch@ 命令即可以应用到源码中, 后续在进行编译测试即可。</p>

<p>另外我们有时会有需求将 "自己使用的 udf 编译到hive当中以方便使用":http://my.oschina.net/wangjiankui/blog/64230 ，这其实也可以看成打补丁的行为。</p>

<p>打补丁多了，管理是后面会遇到的问题。 就个人而言，可以自己载个官方的git仓库，然后开分支来维护自己的变更。如果要多人协作的话，可以创建一个新的git仓库，然后添加两个远端, 其中一个是官方的git仓库，另外一个可以是大家从某个分支开始维护的版本。纯属yy，实际操作起来也未必省心。 这里看起来使用 "github organization":https://github.com/blog/674-introducing-organizations 是蛮适合的。</p>

<p>当然，可以预见的是，大部分时间我们都不会打补丁，可能只会在新增一些UDF= =, 于是似乎完全没有必要这么干, 大概 "类似这样就可以了":https://github.com/nexr/hive-udf , 看起来一般将用到的udf打包，然后要用的时候加一句声明即可。</p>

<p>不过我还是蛋疼去搞了一个 "organization":https://github.com/MTDATA/hive , fork一下官方代码。 我在本地git加入 @apache/hive@ 作为另一个远端, 称为 @mirror@ 。由于我们目前使用的是 0.9, 故我考虑的是在 @origin/branch-0.9@ 下做开发，并将 @mirror/branch-0.9@ 定期 merge 回 @branch-0.9@ 。至于 @trunk@ 则不动。</p>

<p>这样一来我就开始捣鼓了，首先还是改 @build.properties@ , 我将这些也一并提交到分支里面了。 然后我开始将 udf 编译进去，这次遇到一个比较坑爹的问题就是 <em>编译一直成功，但是执行单元测试一直找不到新的UDF</em> 。 捣鼓了很久无果后找小美支援，小美在分析一阵之指出测试的时候没有引用到编译的 @FunctionRegistry.class@ , 估计使用到别的旧的文件了。又过了一段时间之后我确实发现有个地方 @~/.ivy2/@ 下面有一些cache, 简单google之后又发现 @hive jira@ 上已经有一个 "tck":https://issues.apache.org/jira/browse/HIVE-3092 , 看了大概是由 "HIVE-2646":https://issues.apache.org/jira/browse/HIVE-2646 引入的, 导致测试的时候会从ivy cache来加载hive相关的类而不是 @build/dist/lib@ 。这个改动恰恰不在 0.9.0发布版但是 commit到 @branch-0.9@ 中了, 所以我从 @branch-0.9@ 的代码开始编译就中招了。这个patch有加入trunk中，故 @cherry-pick@ 一下即可(本来我打算直接打patch的)。当然这里我不清楚为啥这个补丁不加入 0.9 分支，估计不算大问题? 没有运行bug？从版本库编译就是比较坑爹= =</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[deal with special characters in hive query result]]></title>
    <link href="http://code6.github.com/blog/2012/05/21/deal-with-special-characters-in-hive-query-result/"/>
    <updated>2012-05-21T01:21:00+08:00</updated>
    <id>http://code6.github.com/blog/2012/05/21/deal-with-special-characters-in-hive-query-result</id>
    <content type="html"><![CDATA[<p>p(date). 2012-06-02</p>

<p>知识，想法是需要整理的， 不然容易遗忘, 又过了好久没有更新, 怨念= -
这段时间搞了一点hive之类的东西，遇到一些问题，后续也许还会整理一点出来。
最近遇到的关于hive的问题是:</p>

<p>bq. 将hive查询结果从临时文件导入mysql的时候, 由于查询结果中有特殊字符(比如反斜杠, 制表符等), 导致数据导入mysql时解析出错。</p>

<p>这里得先说明一下mysql导入导出的默认行为。 "mysql命令行输出默认会做转义":http://stackoverflow.com/questions/7287658/mysql-escape-backslash 并且在 "导入的时候默认会将反斜杠转义":http://dev.mysql.com/doc/refman/5.0/en/server-sql-mode.html#sqlmode_no_backslash_escapes , 我们一般不会感觉到这一步。当相同问题发生在hive的查询结果时,有些时候字段末尾或行末尾的 @\@ 会将间隔符 @\t@ 或换行符 @\n@ 转义, 导致导入出错。 如果关闭mysql导入时默认转义的话, 那么字段中包含的间隔符 @\t@ 会导致列数变多，同样出现问题。</p>

<p>十分讨厌hive查询结果中的特殊字符, 究其原因主要是 "hive查询结果目前无法对特殊字符进行转义":https://issues.apache.org/jira/browse/HIVE-692 , 另外比较头疼的是 @hive cli@ 或者 @hiveserver@ 的查询结果中, 默认的字段分隔符都是 @\t@ 且不方便变更。 在解析日志的过程中, 字段中难免包括 @\t@,  @\@ 这类特殊字符。此问题还得仔细对待, 在网上搜了下，大概有几种方法, 未找着较优雅的方案。</p>

<p>h3. 绕过转义问题</p>

<p>一种做法是我们绕过此问题。如果我们不对查询结果进行转义，那么我们就只能让mysql不对导入数据进行转义了( @no_backslash_escapes@ ), 这里需要我们对查询结果给定一个特殊的分隔符，比如 @0x01@ 。</p>

<ul>
<li><em>CTAS</em>
具体内容即</li>
</ul>


<p>bq. When you are doing output to the console \T is your only option. The
best way to handle this is create another table with the delimiters
you wish and then select into that table. You can do this with CTAS.</p>

<p>比如</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>CTAS </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>create table xzy
</span><span class='line'>row format delimited
</span><span class='line'>fields terminated by '\001'
</span><span class='line'>as select age, dt  from ibtest limit 1;</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>这里可以指定任意的分隔符，但这边文件在 @/user/hive/warehouse/xzy@ 中，我们还得将其合并输出到临时文件中。</p>

<ul>
<li>INSERT OVERWRITE LOCAL DIRECTORY</li>
</ul>


<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>local directory </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>INSERT OVERWRITE LOCAL DIRECTORY '/mydir'
</span><span class='line'>SELECT XXX</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>直接写到外部文件夹, 此时分隔符为 @0x01@ 。</p>

<ul>
<li><em>concat_ws</em>
这个算是一个比较取巧的方法吧，xyc介绍的:
将查询结果先转成string, 然后 <em>concat</em> 起来，并指定分隔符。这样由于最终只有一列，所以不会 加上系统默认的分隔符了。</li>
</ul>


<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>concat_ws </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>select concat_ws('\001', cast(userid as string), cast(cityid as string), regdate) from hiveuser limit 10;</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>h3. 勉强进行转义</p>

<p>另外一种做法是勉强在hql中进行转义, 比如将 @\@ 替换成 @\@ , 将制表符替换成 @\@ 和 @t@ :</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>regexp_replace </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>regexp_replace(regexp_replace(column, '\\', '\\\\'), '\t', '\\t')</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>这里得在可能出现特殊字符的地方做上述修改, 为防止写得繁琐，可以考虑封成 @udf@ 。</p>

<p>h3. 总结</p>

<p>总而言之，目前并没想到较好的处理方法。反过来想，类似制表符的特殊字符在日志中是否有意义，如果没有的话，可否去掉?  这样一来就愉快多了，但是 <em>破坏原始日志</em> ，感觉也不是很好。</p>
]]></content>
  </entry>
  
</feed>
