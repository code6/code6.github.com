<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hive | MayCoder]]></title>
  <link href="http://code6.github.com/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://code6.github.com/"/>
  <updated>2013-04-04T12:08:37+08:00</updated>
  <id>http://code6.github.com/</id>
  <author>
    <name><![CDATA[Code6]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[open hive]]></title>
    <link href="http://code6.github.com/blog/2012/12/02/open-hive/"/>
    <updated>2012-12-02T21:48:00+08:00</updated>
    <id>http://code6.github.com/blog/2012/12/02/open-hive</id>
    <content type="html"><![CDATA[<p>p(date). 2013-01-05</p>

<p>有时我说的话就是放屁，这已经都到 -12月- 1月了。不过我还是厚着脸皮来了，我还是希望写写的，每当我来refer自己干过的一些事情的时候, 还蛮有用的。</p>

<p>今天我想说的是近期我们在做的一个事情，说来也简单，往往我觉得难的不是要写多高深的代码，而是采取什么方式来达到一个目的, 满足需求。我的条理还有待加强，暂且这样。</p>

<p>对于做数据的团队，一个很重要的事情就是开放数据，怎样提供一个更好的环境来让更多的人来访问数据，获取他们关心的内容。如果数据始终是仅在团队内部可以使用，那么势必会增加很多重复的无谓的体力劳动。在开放数据这一块，之前大家做了一些努力，有了一个内部的自助查询的系统，面向RD和一般的分析人员。开放的方式是提供一个窗口让用户写具体的SQL来查询，返回查询结果, 并可以定制很多图表。 系统内开放了若干链接，每个链接可以查若干表，并可以看到表相关的字段信息。一般来说这样就够了，当然写SQL门槛稍稍有点高。</p>

<p>之前主要数据都是在 mysql 上，开放的方式也非常简单, 只需要提供一个从库即可, 让查询都落在从库上，然后主库进行更新。当我们在mysql撑不住，逐步迁移到hive的同时，自助查询这边相应的也需要改进以支持hive查询。虽然已经有包括 hwi( "anjuke的改进版hwi":https://github.com/anjuke/hwi ) 和 phphiveadmin 以及 hue(我们用apache hadoop, 用不了cloudera的产品)了，感觉跟我们自己的查询工具结合在一起会比较方便使用。结合的方式也比较简单, 简单使用hive cli的执行方式来查询并保存结果数据即可, 大致如下:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>run hive query  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>hive -f /tmp/testsql 1&gt;testsql.output 2&gt;testsql.err
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>这样把查询结果放在临时文件中，然后再展示给用户。另外由于hive查询一般比较久，故只能做成异步查询。</p>

<p>这里为什么不用hiveserver呢? 感觉hiveserver在我们这边使用总有一些问题，使用hive 0.9 配合zookeeper时，hiveserver看起来会泄露zookeeper连接，导致一段时间后就不可用了。 当然也可能是我们的姿势不对, 不知在 hive 0.10 上是否会好些。</p>

<p>开放查询做到这一步其实基本差不多了, 用户可以自己执行查询并查看/下载结果集。 但我们在这一步的时候却还不能开放，觉得对用户的控制还不够，用户权限太大了。 说到这一点，我们目前的hadoop/hive环境上只有一个默认用户，其余用于查询的用户(比如apache)基本上对所有表都有查询权限。另外一点是我们线上的hive任务也在定期执行，自助查询这边不应该占用过多资源。在一些必要场合下我们必须有能力干掉用户的查询。简单来说，初步开放，我们需要做到:
<em>必要的权限控制</em>
库/表级别，语句类型(限制只能使用select)，这些控制可以跟账号绑定。
<em>资源限制</em>
任务同时可以起的map数量和reduce数量控制。
<em>锁限制</em>
自助查询不能争用线上任务的锁。</p>

<ul>
<li><em>必要的权限控制</em>
在做权限控制的时候本来想直接用 @hive.security.authorization.enabled@ 参数来搞，发现当所有用户都拿一个账号(apache)来访问是没啥用的= =, 没办法，只能手动搞了，于是比较糙地搞了个类似的自己用, 表如下:</li>
</ul>


<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>hivetablepriv   </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="n">REATE</span> <span class="k">TABLE</span> <span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">hivetablepriv</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span> <span class="p">(</span>
</span><span class='line'>  <span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">grantid</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span> <span class="nb">int</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="n">unsigned</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="n">AUTO_INCREMENT</span> <span class="k">COMMENT</span> <span class="s1">&#39;授权自增ID&#39;</span><span class="p">,</span>
</span><span class='line'>  <span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">rolename</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">DEFAULT</span> <span class="s1">&#39;&#39;</span> <span class="k">COMMENT</span> <span class="s1">&#39;授权角色名&#39;</span><span class="p">,</span>
</span><span class='line'>  <span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">dbname</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">DEFAULT</span> <span class="s1">&#39;&#39;</span> <span class="k">COMMENT</span> <span class="s1">&#39;授权库名&#39;</span><span class="p">,</span>
</span><span class='line'>  <span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">tablename</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">DEFAULT</span> <span class="s1">&#39;&#39;</span> <span class="k">COMMENT</span> <span class="s1">&#39;授权表名, *表示任意表&#39;</span><span class="p">,</span>
</span><span class='line'>  <span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">modtime</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span> <span class="n">datetime</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">DEFAULT</span> <span class="s1">&#39;0000-00-00 00:00:00&#39;</span> <span class="k">COMMENT</span> <span class="s1">&#39;记录修改时间&#39;</span><span class="p">,</span>
</span><span class='line'>  <span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">status</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span> <span class="n">tinyint</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="n">unsigned</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">DEFAULT</span> <span class="s1">&#39;0&#39;</span> <span class="k">COMMENT</span> <span class="s1">&#39;0为正常授权，128为已删除授权&#39;</span><span class="p">,</span>
</span><span class='line'>  <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">grantid</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span><span class="p">),</span>
</span><span class='line'>  <span class="k">UNIQUE</span> <span class="k">KEY</span> <span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">rolename</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">rolename</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span><span class="p">,</span><span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">dbname</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span><span class="p">,</span><span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">tablename</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span><span class="p">)</span>
</span><span class='line'><span class="p">)</span> <span class="n">ENGINE</span><span class="o">=</span><span class="n">InnoDB</span> <span class="n">AUTO_INCREMENT</span><span class="o">=</span><span class="mi">62</span> <span class="k">DEFAULT</span> <span class="n">CHARSET</span><span class="o">=</span><span class="n">utf8</span> <span class="k">COMMENT</span><span class="o">=</span><span class="s1">&#39;hive开放查询权限表&#39;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>然后在实际查询的时候，使用虚拟角色来访问，解析一下查询语句使用到的表(简单正则)，然后进行一下权限判断即可。</p>

<ul>
<li><p><em>资源限制</em>
这步可以很简单做，比如直接在 @Hadoop Fair Scheduler@ 配置一个受限的队列即可, 基本可以保证资源限制。</p></li>
<li><p><em>锁限制</em>
我们打开了 @hive.support.concurrency@ 并使用zookeeper来进行锁竞争，主要是防止线上日常ETL程序执行过程中对资源有竞争(比如一个表在更新时有流程访问)，这里在自己查询避免争用线上任务的锁只需要关闭此开关即可。</p></li>
</ul>


<p>综上, 我们仅需把执行语句加强一下:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>run hive query safely  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>hive --hiveconf hive.support.concurrency<span class="o">=</span><span class="nb">false</span> -hiveconf mapred.queue.name<span class="o">=</span>slow -f /tmp/testsql 1&gt;testsql.output 2&gt;testsql.err
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>另外权限控制我们在查询脚本执行查询之前做。我们还缺的一件事情是记录下用户的查询(logging)。这一步也可以简单在hive执行脚本中收集。为了能够更好的监控这些任务，我们还需要能够将查询和其所起的任务关联起来。这里使用了一个比较土的方法，就是边执行边解析程序的标准错误, 收集日志中提到的起的hadoop jobid。有了这些信息之后，就可以将一个查询所对应的任务干掉了。</p>

<p>这样我们只靠一个简单查询脚本就可以基本的给hive查询一个具有较好约束的执行环境了。这个openhive脚本可以直接给自助查询工具查询hive使用。不过对于其他RD有需求查询hive数据的，我们还得暴露一个较友好的接口，比如使用 @thrift@ 做一个服务。想到这边我又想到 @hiveserver@ 了, 感觉我的想法有点多余。不过还是那个问题，我们希望有更多控制，然后又不太想在hive源码上动手脚(觉得麻烦以及以后升级比较有问题, 当然主要是我对java不熟)，所以搭 @thrift@ 然后背后用调用 openhive 脚本来实现, 接口可以这样:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>openhive.thrift   </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="k">struct</span> <span class="n">Query</span> <span class="p">{</span>
</span><span class='line'>  <span class="mi">1</span><span class="o">:</span> <span class="n">required</span> <span class="n">string</span> <span class="n">query</span><span class="p">,</span>
</span><span class='line'>  <span class="mi">2</span><span class="o">:</span> <span class="n">required</span> <span class="n">string</span> <span class="n">username</span><span class="p">,</span>
</span><span class='line'>  <span class="mi">3</span><span class="o">:</span> <span class="n">optional</span> <span class="n">string</span> <span class="n">password</span>
</span><span class='line'><span class="p">}</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">exception</span> <span class="n">OpenHiveException</span> <span class="p">{</span>
</span><span class='line'>  <span class="mi">1</span><span class="o">:</span> <span class="n">string</span> <span class="n">message</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'><span class="n">service</span> <span class="n">OpenHive</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="cp"># Execute a query. Takes a HiveQL string</span>
</span><span class='line'>  <span class="kt">void</span> <span class="n">query</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">Query</span> <span class="n">q</span><span class="p">)</span> <span class="n">throws</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">OpenHiveException</span> <span class="n">ex</span><span class="p">)</span>
</span><span class='line'>  <span class="cp"># Fetch one row. This row is the serialized form</span>
</span><span class='line'>  <span class="cp"># of the result of the query</span>
</span><span class='line'>  <span class="n">string</span> <span class="n">fetchOne</span><span class="p">()</span> <span class="n">throws</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">OpenHiveException</span> <span class="n">ex</span><span class="p">)</span>
</span><span class='line'>  <span class="cp"># Fetch a given number of rows or remaining number of</span>
</span><span class='line'>  <span class="cp"># rows whichever is smaller.</span>
</span><span class='line'>  <span class="n">list</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">fetchN</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">i32</span> <span class="n">numRows</span><span class="p">)</span> <span class="n">throws</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">OpenHiveException</span> <span class="n">ex</span><span class="p">)</span>
</span><span class='line'>  <span class="cp"># Fetch all rows of the query result</span>
</span><span class='line'>  <span class="n">list</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">fetchAll</span><span class="p">()</span> <span class="n">throws</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">OpenHiveException</span> <span class="n">ex</span><span class="p">)</span>
</span><span class='line'>  <span class="cp"># Fetch Query header</span>
</span><span class='line'>  <span class="n">string</span> <span class="n">fetchQueryHeader</span><span class="p">()</span> <span class="n">throws</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">OpenHiveException</span> <span class="n">ex</span><span class="p">)</span>
</span><span class='line'>  <span class="cp"># Fetch all query log</span>
</span><span class='line'>  <span class="n">string</span> <span class="n">fetchQueryLog</span><span class="p">()</span> <span class="n">throws</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">OpenHiveException</span> <span class="n">ex</span><span class="p">)</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>整个接口基本是 @hiveserver@ 的子集= = 服务端简单使用Python来搞，使用thrift的 @ProcessPoolServer@ 类 ，其原理是server启动时fork出N个工作进程，进程之间不影响且可以重复使用。 这样遇到的问题是服务器的处理有瓶颈，每次最多同时处理N个任务，来多了会阻塞，目前我暂时将N设置为10。后续完全可以做成异步的形式，当然得修改接口，比如添加一个 @sessionid@  的概念。后来跟同事商量，其实对于hive查询本身server端是没有什么压力的(主要在hadoop集群上), 故基本不需要考虑 @python GIL@ 问题而使用多进程。另外他建议使用 @gevent@ 来提高server的并发能力，当然这样瓶颈就落到后端查询上了，感觉还是需要一个队列的机制，倒是可以试试。</p>

<p>大概就是这样了，废话了很多其实内容很少，也很简单。条理和叙述方式有待加强，终于又写了一篇blog，欢迎交流反馈~~</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hive compile and apply patch]]></title>
    <link href="http://code6.github.com/blog/2012/09/19/hive-compile-and-apply-patch/"/>
    <updated>2012-09-19T13:38:00+08:00</updated>
    <id>http://code6.github.com/blog/2012/09/19/hive-compile-and-apply-patch</id>
    <content type="html"><![CDATA[<p>p(date). 2012-09-20</p>

<p>回归来的第一篇，另外还有好几篇躺在 @published:false@ 中，还没搞好 :-(</p>

<p>这个礼拜最大的收获是编译啥啥啥还是蛮简单的，又没叫你去改代码。有些东西动手尝试一下，往往会发现没有多难，不能停留在口上上说说而已。看到 "一段相关但不是很切题的话":http://blog.liancheng.info/wq-2008-1/ :</p>

<p>bq. 不可忽视的一点是：在技术性团体里，做永远比说要来的有效。在想法成熟之前就贸然游说，往往会招致相反的效果。大家都是工程师，不会贸然接纳陌生事物。如果自己都还没有想清楚就开始大肆游说，往往会被大家提出的实际的工程问题驳斥地体无完肤。当你哑口无言之时，大家也已经对你的方案产生了难以磨灭的 “不靠谱”的第一印象，这时要再想咸鱼翻身，可就没那么容易了。</p>

<p>bq. 相对的，首先自行查阅文档资料并进行试验，制作demo，通过试验发现和解决实际出现的问题。在想法基本成型时，和个别观念开放的同仁进行探讨，这时往往可以发现大量之前自己没有考虑到的问题，再转而细化方案。这个过程反复迭代几次之后，方案和demo逐渐成熟，同时也潜移默化地达到了传教的目的。等到方案完全成熟之后，再拿出实际可工作的demo开始游说，这时自然就成竹在胸了。</p>

<p>进入正题，这里主要是参考几篇文章，说说编译hive和打补丁, 或者说贴贴链接= =</p>

<p>h3. hive 代码编译</p>

<p>"install hive on linux":http://railsbuilder.blogspot.com/2010/06/installing-hive-on-linux.html<br/>
"使用ant编译hive":http://blog.csdn.net/scutshuxue/article/details/5915689<br/>
"hive打补丁编译hive":http://blog.csdn.net/cfy_yinwenhao/article/details/6977882</p>

<p>这里我在ubuntu上操作的, 编译hive需要依赖 JAVA, ANT (这里不依赖hadoop原因是ant会自动去下载hadoop)。
安装好后export @JAVA_HOME@ , @ANT_HOME@ 变量 (我直接加入.bashrc当中了)。
接下来去官网下载hive源码包。这里我使用的是 @hive.0.9.0@ 。 解压之后进入src目录，需要修改一些配置:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>build.properties </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='properties'><span class='line'><span class="na">hadoop.version</span><span class="o">=</span><span class="s">1.0.1&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="na">&lt;p&gt;主要是facebook被墙了，镜像地址可参考 http</span><span class="o">:</span><span class="s">//www.apache.org/dyn/closer.cgi/hadoop/core/</span>
</span><span class='line'><span class="na">hadoop.mirror1</span><span class="o">=</span><span class="s">http://mirror.bjtu.edu.cn/apache&lt;br/&gt;</span>
</span><span class='line'><span class="na">hadoop.mirror2</span><span class="o">=</span><span class="s">http://mirror.bjtu.edu.cn/apache&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="err">&lt;p&gt;以下的修改主要是看到镜像中的hadoop版本已经没有默认的版本号了，估计是不支持了?故改成包含的版本号。</span>
</span><span class='line'><span class="na">hadoop-0.20.version</span><span class="o">=</span><span class="s">0.20.2</span>
</span><span class='line'><span class="na">hadoop-0.20S.version</span><span class="o">=</span><span class="s">1.0.1 (S的版本不知有没有啥区别)&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="err">&lt;p&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>接下来执行 @ant package@ 即开始编译了, 结果默认会放在 @build/dist@ 目录中，使用 @-Dtarget.dir=xxx@ 可以更新目标位置。</p>

<p>如果BUILD FAIL的话，一般参考提示可以看出是哪里出了问题(遇到比较多还是被墙的问题= =)。</p>

<p>h3. hive 单元测试</p>

<p>"hive中的单元测试":http://www.oratea.net/?p=632
"hive中的test case":http://blog.csdn.net/bupt041137/article/details/6553770
"QtestUtil.java":http://blog.csdn.net/bupt041137/article/details/6553760
"hive unit test":http://blog.csdn.net/bupt041137/article/details/6553760</p>

<p>hive 中的 "单元测试":https://cwiki.apache.org/Hive/howtocontribute.html#HowToContribute-UnitTests 还是蛮有趣的，既有传统的单元测试代码，又可以批量执行查询脚本判断结果是否一致:</p>

<p>bq.  在hive中会有大量的.q的文件即执行测试的query文件，.q.out是测试的结果文件。 进行新测试后产生的结果和标准的q.out一致则表示测试成功。
具体来说，src/ql/src/test/queries下面是测试用例，clientpositive是运行成功的用例，clientnegative是运行失败，返回非0的用例。 src/ql/src/test/results 下面是测试用例对应的输出结果。 如src/ql/src/test/queries/case_sensitivity.q对应的输出结果是src/ql/src/test/results/case_sensitivity.q.out 。</p>

<p>使用 @ant test@ 运行单元测试，不过此命令要跑好多测试， "光TestCliDriver一项就要跑好几个小时":https://builds.apache.org/job/Hive-trunk-h0.21/1671/testReport/org.apache.hadoop.hive.cli/TestCliDriver/ , 我们可以</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>ant test </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>运行所有的正面的测试
</span><span class='line'>ant test -Dtestcase=TestCliDriver&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>运行特定的测试，以groupby为例子
</span><span class='line'>ant test -Dtestcase=TestCliDriver -Dqfile=groupby1.q</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>使用 @-Doverwrite=true@ 可以覆盖结果文件，后面有新的更新想测试表现是否一致的时候就可以使用了, 通常在我们增加新的udf的时候可以这么来生成测试数据。</p>

<p>h3. hive代码打补丁</p>

<p>下载了 @hive 0.9.0@ 稳定版，不过发布之后还是有若干bug，这时搜索一下 @hive jira@ 一般能看到相应ticket，比如 "这个substr遇到UTF8的问题":https://issues.apache.org/jira/browse/HIVE-2942 。一般打的补丁会走一个 @publish-review-apply@ 的 "过程":http://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;cad=rja&amp;ved=0CEIQFjAD&amp;url=http%3A%2F%2Fwww.cs.pdx.edu%2F~bart%2Fpapers%2Fpicmet-patch.pdf&amp;ei=g0VbUK6LDITqmAW2tICQCQ&amp;usg=AFQjCNEoh5FKA_8KNBa5-Pr-gMaQPHS9GA&amp;sig2=ISd1WAautuWBKKfc_Vv3lg ，准确解决问题的补丁会被加入版本库中，而临时补丁一般不会采用。作为用户，有时难免得打一些这样的补丁。打补丁也很方便，下载社区提供的补丁之后，使用 @patch@ 命令即可以应用到源码中, 后续在进行编译测试即可。</p>

<p>另外我们有时会有需求将 "自己使用的 udf 编译到hive当中以方便使用":http://my.oschina.net/wangjiankui/blog/64230 ，这其实也可以看成打补丁的行为。</p>

<p>打补丁多了，管理是后面会遇到的问题。 就个人而言，可以自己载个官方的git仓库，然后开分支来维护自己的变更。如果要多人协作的话，可以创建一个新的git仓库，然后添加两个远端, 其中一个是官方的git仓库，另外一个可以是大家从某个分支开始维护的版本。纯属yy，实际操作起来也未必省心。 这里看起来使用 "github organization":https://github.com/blog/674-introducing-organizations 是蛮适合的。</p>

<p>当然，可以预见的是，大部分时间我们都不会打补丁，可能只会在新增一些UDF= =, 于是似乎完全没有必要这么干, 大概 "类似这样就可以了":https://github.com/nexr/hive-udf , 看起来一般将用到的udf打包，然后要用的时候加一句声明即可。</p>

<p>不过我还是蛋疼去搞了一个 "organization":https://github.com/MTDATA/hive , fork一下官方代码。 我在本地git加入 @apache/hive@ 作为另一个远端, 称为 @mirror@ 。由于我们目前使用的是 0.9, 故我考虑的是在 @origin/branch-0.9@ 下做开发，并将 @mirror/branch-0.9@ 定期 merge 回 @branch-0.9@ 。至于 @trunk@ 则不动。</p>

<p>这样一来我就开始捣鼓了，首先还是改 @build.properties@ , 我将这些也一并提交到分支里面了。 然后我开始将 udf 编译进去，这次遇到一个比较坑爹的问题就是 <em>编译一直成功，但是执行单元测试一直找不到新的UDF</em> 。 捣鼓了很久无果后找小美支援，小美在分析一阵之指出测试的时候没有引用到编译的 @FunctionRegistry.class@ , 估计使用到别的旧的文件了。又过了一段时间之后我确实发现有个地方 @~/.ivy2/@ 下面有一些cache, 简单google之后又发现 @hive jira@ 上已经有一个 "tck":https://issues.apache.org/jira/browse/HIVE-3092 , 看了大概是由 "HIVE-2646":https://issues.apache.org/jira/browse/HIVE-2646 引入的, 导致测试的时候会从ivy cache来加载hive相关的类而不是 @build/dist/lib@ 。这个改动恰恰不在 0.9.0发布版但是 commit到 @branch-0.9@ 中了, 所以我从 @branch-0.9@ 的代码开始编译就中招了。这个patch有加入trunk中，故 @cherry-pick@ 一下即可(本来我打算直接打patch的)。当然这里我不清楚为啥这个补丁不加入 0.9 分支，估计不算大问题? 没有运行bug？从版本库编译就是比较坑爹= =</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[deal with special characters in hive query result]]></title>
    <link href="http://code6.github.com/blog/2012/05/21/deal-with-special-characters-in-hive-query-result/"/>
    <updated>2012-05-21T01:21:00+08:00</updated>
    <id>http://code6.github.com/blog/2012/05/21/deal-with-special-characters-in-hive-query-result</id>
    <content type="html"><![CDATA[<p>p(date). 2012-06-02</p>

<p>知识，想法是需要整理的， 不然容易遗忘, 又过了好久没有更新, 怨念= -
这段时间搞了一点hive之类的东西，遇到一些问题，后续也许还会整理一点出来。
最近遇到的关于hive的问题是:</p>

<p>bq. 将hive查询结果从临时文件导入mysql的时候, 由于查询结果中有特殊字符(比如反斜杠, 制表符等), 导致数据导入mysql时解析出错。</p>

<p>这里得先说明一下mysql导入导出的默认行为。 "mysql命令行输出默认会做转义":http://stackoverflow.com/questions/7287658/mysql-escape-backslash 并且在 "导入的时候默认会将反斜杠转义":http://dev.mysql.com/doc/refman/5.0/en/server-sql-mode.html#sqlmode_no_backslash_escapes , 我们一般不会感觉到这一步。当相同问题发生在hive的查询结果时,有些时候字段末尾或行末尾的 @\@ 会将间隔符 @\t@ 或换行符 @\n@ 转义, 导致导入出错。 如果关闭mysql导入时默认转义的话, 那么字段中包含的间隔符 @\t@ 会导致列数变多，同样出现问题。</p>

<p>十分讨厌hive查询结果中的特殊字符, 究其原因主要是 "hive查询结果目前无法对特殊字符进行转义":https://issues.apache.org/jira/browse/HIVE-692 , 另外比较头疼的是 @hive cli@ 或者 @hiveserver@ 的查询结果中, 默认的字段分隔符都是 @\t@ 且不方便变更。 在解析日志的过程中, 字段中难免包括 @\t@,  @\@ 这类特殊字符。此问题还得仔细对待, 在网上搜了下，大概有几种方法, 未找着较优雅的方案。</p>

<p>h3. 绕过转义问题</p>

<p>一种做法是我们绕过此问题。如果我们不对查询结果进行转义，那么我们就只能让mysql不对导入数据进行转义了( @no_backslash_escapes@ ), 这里需要我们对查询结果给定一个特殊的分隔符，比如 @0x01@ 。</p>

<ul>
<li><em>CTAS</em>
具体内容即</li>
</ul>


<p>bq. When you are doing output to the console \T is your only option. The
best way to handle this is create another table with the delimiters
you wish and then select into that table. You can do this with CTAS.</p>

<p>比如</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>CTAS </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>create table xzy
</span><span class='line'>row format delimited
</span><span class='line'>fields terminated by '\001'
</span><span class='line'>as select age, dt  from ibtest limit 1;</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>这里可以指定任意的分隔符，但这边文件在 @/user/hive/warehouse/xzy@ 中，我们还得将其合并输出到临时文件中。</p>

<ul>
<li>INSERT OVERWRITE LOCAL DIRECTORY</li>
</ul>


<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>local directory </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>INSERT OVERWRITE LOCAL DIRECTORY '/mydir'
</span><span class='line'>SELECT XXX</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>直接写到外部文件夹, 此时分隔符为 @0x01@ 。</p>

<ul>
<li><em>concat_ws</em>
这个算是一个比较取巧的方法吧，xyc介绍的:
将查询结果先转成string, 然后 <em>concat</em> 起来，并指定分隔符。这样由于最终只有一列，所以不会 加上系统默认的分隔符了。</li>
</ul>


<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>concat_ws </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>select concat_ws('\001', cast(userid as string), cast(cityid as string), regdate) from hiveuser limit 10;</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>h3. 勉强进行转义</p>

<p>另外一种做法是勉强在hql中进行转义, 比如将 @\@ 替换成 @\@ , 将制表符替换成 @\@ 和 @t@ :</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>regexp_replace </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>regexp_replace(regexp_replace(column, '\\', '\\\\'), '\t', '\\t')</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>这里得在可能出现特殊字符的地方做上述修改, 为防止写得繁琐，可以考虑封成 @udf@ 。</p>

<p>h3. 总结</p>

<p>总而言之，目前并没想到较好的处理方法。反过来想，类似制表符的特殊字符在日志中是否有意义，如果没有的话，可否去掉?  这样一来就愉快多了，但是 <em>破坏原始日志</em> ，感觉也不是很好。</p>
]]></content>
  </entry>
  
</feed>
